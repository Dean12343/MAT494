{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJKzjVcnJ4WwJUn8zHRJZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dean12343/MAT494/blob/main/MAT494_3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation Algorithm\n",
        "\n",
        "We can easily derive the backpragation algorithm as follows. First we initialize weights and biases, typically at random. Then pick input data and input the vector x into the left side of the network, and calculate all the $z_s,a_s$,etc. And finally calculate the output $\\hat{y}$. We now can update the parameters by the (stochastic) gradient descent. Repeat the process until the desired accuracy is reached. For example, if using the quadratic cost\n",
        "function in one dimension, then\n",
        "\n",
        "$Î´^{(L)}=\\frac{dg^{(L)}}{dz}|_{z_j^L}(\\hat{y}-y)$\n",
        "\n",
        "\n",
        "Then update the weights and biases using the following formulas.\n",
        "\n",
        "$New\\; w_{j,j'}^{(l)}=Old\\;w_{j,j'}^{(l)}-\\beta\\frac{\\partial J}{\\partial w_{j,j'}^{(l)}}$\n",
        "\n",
        "and\n",
        "\n",
        "$New\\; b_{j'}^{(l)}=Old\\;b_{j'}^{(l)}-\\beta\\frac{\\partial J}{\\partial b_{j'}^{(l)}}$\n",
        "\n",
        "Problem 1: Initialize a neural network and print the hidden layer with one neuron, 2 input weights, and a bias as well as an output layer with 2 neurons each with 2 weights, and 1 bias."
      ],
      "metadata": {
        "id": "OvNNhQQGbLnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import random\n",
        " \n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 2)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network\n",
        " \n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFC3VRIgSl6p",
        "outputId": "104ee2c3-2749-477b-f9cf-30ab52125a40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}, {'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}]\n"
          ]
        }
      ]
    }
  ]
}